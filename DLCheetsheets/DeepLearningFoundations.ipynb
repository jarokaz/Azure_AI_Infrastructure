{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Foundations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is deep learning?\n",
    "\n",
    "\n",
    "![Deep Learning](images/dl.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why deep representation?\n",
    "\n",
    "Circuit theory proves that there are functions that can be computed with \"narrow\" (relatively small number of hidden units in a layer) but deep (many layers) that shallower networks require exponentially more hidden units to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks as computational graphs\n",
    "\n",
    "![Computational graphs](images/graphs.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network training\n",
    "\n",
    "![ANN training](images/training.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network representation\n",
    "\n",
    "![Representation](images/representation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural network representation - vector notation\n",
    "\n",
    "![Representation](images/representation2.PNG)\n",
    "\n",
    "Let\n",
    "\n",
    "$\\mathbf X =\\begin{bmatrix} | & | &   & | \\\\ \n",
    "                \\mathbf x^{(1)} & \\mathbf x^{(2)} & \\cdots & \\mathbf x^{(m)} \\\\\n",
    "                          | & | &   & | \\end{bmatrix},\\qquad$\n",
    "$\\mathbf x^{(i)} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_{n^{[0]}}^{(i)} \\end{bmatrix},\\qquad$\n",
    "$\\mathbf X\\in\\mathbb R^{n^{[0]} \\times m},\\qquad$\n",
    "$\\mathbf x\\in\\mathbb R^{n^{[0]}}$\n",
    "\n",
    "$\\mathbf A^{[l]}=\\begin{bmatrix} | & | &   & | \\\\ \n",
    "                \\mathbf a^{[l](1)} & \\mathbf a^{[l](2)} & \\cdots & \\mathbf a^{[l](m)} \\\\\n",
    "                          | & | &   & | \\end{bmatrix},\\qquad$\n",
    "$\\mathbf a^{[l](i)} = \\begin{bmatrix} a_1^{[l](i)} \\\\ a_2^{[l](i)} \\\\ \\vdots \\\\ a_{n^{[l]}}^{[l](i)} \\end{bmatrix},\\qquad$       $\\mathbf A\\in\\mathbb R^{n^{[l]} \\times m},\\qquad$\n",
    "$\\mathbf a\\in\\mathbb R^{n^{[l]}}$           \n",
    "                          \n",
    "$\\mathbf W^{[l]}=\\begin{bmatrix} -\\mathbf w_{1}^{[l]}- \\\\ \n",
    "                                 -\\mathbf w_{2}^{[l]}- \\\\ \n",
    "                                 \\vdots \\\\ \n",
    "                                 -\\mathbf w_{n^{[l]}}^{[l]}- \\\\ \n",
    "                 \\end{bmatrix},\\qquad$\n",
    "$\\mathbf w_{i}^{[l]}=[w_1^{[l]},w_2^{[l]}, \\cdots, w_{n^{[l-1]}}^{[l]}],\\qquad$\n",
    "$\\mathbf b^{[l]}=\\begin{bmatrix} b_{1}^{[l]} \\\\ \n",
    "                                 b_{2}^{[l]} \\\\ \n",
    "                                 \\vdots \\\\ \n",
    "                                 b_{n^{[l]}}^{[l]} \\\\ \n",
    "                 \\end{bmatrix},\\qquad$ \n",
    "$\\mathbf W\\in\\mathbb R^{n^{[l]} \\times n^{[l-1]}},\\qquad$\n",
    "$\\mathbf b\\in\\mathbb R^{n^{[l]}}$   \n",
    "                          \n",
    "then               \n",
    "\n",
    "$\\mathbf Z^{[1]}=\\mathbf W^{[1]}\\mathbf X + \\mathbf b^{[1]},\\qquad$\n",
    "$\\mathbf A^{[1]}=\\sigma(\\mathbf Z^{[1]})$\n",
    "\n",
    "$\\mathbf Z^{[2]}=\\mathbf W^{[2]}\\mathbf A^{[1]} + \\mathbf b^{[2]},\\qquad$\n",
    "$\\mathbf A^{[2]}=\\sigma(\\mathbf Z^{[2]})$\n",
    "\n",
    "$\\mathbf Z^{[l]}=\\mathbf W^{[l]}\\mathbf A^{[l-1]} + \\mathbf b^{[l]},\\qquad$\n",
    "$\\mathbf A^{[l]}=\\sigma(\\mathbf Z^{[l]})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "\n",
    "![Activation functions](images/activations.PNG)\n",
    "\n",
    "$\\qquad\\qquad a=\\dfrac{1}{1+e^{-z}}\\qquad\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\dfrac{e^z-e^{-z}}{e^z+e^{-z}}\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\max(0,z)\\qquad\\qquad\\qquad\\qquad$\n",
    "$a=\\max(\\alpha z,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output units\n",
    "\n",
    "Let's assume that the hidden layers compute a set of hidden features defined by\n",
    "\n",
    "$\\mathbf h=f(\\mathbf x; \\boldsymbol \\Theta),\\qquad$where $\\boldsymbol \\Theta$ is a set of all parameter tensors in hidden layers $\\boldsymbol \\Theta=\\{\\mathbf W^{[1]},...,\\mathbf W^{[L_h]},\\mathbf b^{[1]},...,\\mathbf b^{[L_h]}\\}$\n",
    "\n",
    "The role of the ouput layer is then to provide some additional transformation from the features to complete the task that the network must perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear units\n",
    "\n",
    "Given features $\\mathbf h$, a layer linear output units produces a vector\n",
    "\n",
    "$\\hat y=\\mathbf W^T\\mathbf h + \\mathbf b, \\quad$ where $\\mathbf W, \\mathbf b$ are parameter tensors of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sigmoid units for binary classification tasks\n",
    "\n",
    "In the case of binary classification the output layer comprises a single unit.\n",
    "\n",
    "Given features $\\mathbf h$, a sigmoid output unit produces a scalar value that can be interpreted as a probability of a positive class. \n",
    "\n",
    "$\\hat y=\\sigma(\\mathbf w^T\\mathbf h + \\mathbf b), \\quad$ where $\\mathbf w, \\mathbf b$ are parameter vectors of the output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Softmax units for multiclass classification tasks\n",
    "\n",
    "\n",
    "Given features $\\mathbf h$, a softmax output layer produces a vector $\\hat{\\mathbf y}$ that can be interpreted as the probablity distribution over $k$ different classes. \n",
    "\n",
    "Let\n",
    "\n",
    "$\\mathbf z = \\mathbf W^T\\mathbf h+\\mathbf b, \\quad$ where $\\mathbf W, \\mathbf b$ are parameter tensors of the output layer.\n",
    "\n",
    "$\\hat{y_i}=softmax(\\mathbf z)_i=\\dfrac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}, \\quad$ where $\\mathbf w, \\mathbf b$ are parameter vectors of the output unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other output types\n",
    "\n",
    "The linear, sigmoid, and softmax output units are the most common. However; neural networks can generalize to almost any kind of output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost functions\n",
    "\n",
    "An important aspect of the design of a deep neural network is the choice of the cost function. The cost functions for neural networks are more or less the same as those for other parametric models.\n",
    "\n",
    "Most modern neural networks are trained using maximum likelihood. This meand that the cost function is simply the negative log-ikelihood, equivalenty described as the cross-entropy between the training data and the model distribution.\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\mathbb E_{\\mathbf x, \\mathbf y \\sim \\hat p_{data}} \\log p_{model}(\\mathbf y|\\mathbf x)$\n",
    "\n",
    "The specific form of the cost function depends on the form of $\\log p_{model}$ and as such on the type of units used in the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy cost for binary classification\n",
    "\n",
    "Let $\\mathcal D=\\{(\\mathbf x^{(1)},y^{(1)}),...,(\\mathbf x^{(m)},y^{(m)})\\}$ be a training data set and $\\hat y(\\mathbf x)$ the ouput of the network.\n",
    "\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\dfrac{1}{m}\\sum_{i=1}^m(y^{(i)} \\log(\\hat y^{(i)}) + (1-y^{(i)}) \\log (1-\\hat y^{(i)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy cost for multiclass classification\n",
    "\n",
    "Let $\\mathcal D=\\{(\\mathbf x^{(1)}, \\mathbf y^{(1)}),...,(\\mathbf x^{(m)}, \\mathbf y^{(m)})\\}$ be a training data set and $\\hat {\\mathbf y(\\mathbf x)}$ the ouput of the network.\n",
    "\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=-\\dfrac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^C y_k^{(i)} \\log(\\hat y_k^{(i)}),\\quad $ where $C$ is the number of classes (the size of the output vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "To make it easier to understand, the following description shows how to calculate the gradient on a single training tuple $(\\mathbf x, \\mathbf y)$. In practice, backpropagation is vectorized and calculations are done on a whole minibatch. \n",
    "\n",
    "\n",
    "1.__Set the activations for the input layer__ $l=0$\n",
    "\n",
    "$\\qquad \\mathbf a^{[0]}=\\mathbf x$\n",
    "\n",
    "2.__Feedforward__: For each layer $l=1,2,...,L$: compute and cache:\n",
    "\n",
    "$\\qquad \\mathbf z^{[l]}=\\mathbf W^{[l]}\\mathbf a^{[l-1]}+\\mathbf b^{[l]},\\,$ and $\\mathbf a^{[l]}=g(\\mathbf z^{[l]})$\n",
    "\n",
    "3.__Compute the output error__ $\\boldsymbol \\delta^L$: \n",
    "\n",
    "$\\qquad \\boldsymbol \\delta^L = \\nabla_a J \\odot g^{'}(\\mathbf z^L)$\n",
    "\n",
    "4.__Backpropagate the error__: For each layer $l=L-1,L-2,...,1$ compute:\n",
    "\n",
    "$\\qquad \\boldsymbol \\delta^l = ((\\mathbf W^{l+1})^T \\boldsymbol \\delta^{l+1}) \\odot \\boldsymbol \\delta^{'} (\\mathbf z^l)$\n",
    "\n",
    "5.__Calculate gradient__:\n",
    "\n",
    "$\\qquad \\dfrac{\\partial J}{w_{ij}^l}=a_j^{[l-1]}\\delta_i^l\\quad$ and $\\dfrac{\\partial J}{b_{i}^l}=\\delta_i^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivates of activaton functions\n",
    "\n",
    "__Sigmoid__\n",
    "\n",
    "\n",
    "\n",
    "$g(z)=\\dfrac{1}{1+e^{-z}} \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=g(z)(1-g(z))$\n",
    "\n",
    "$g(z)=\\dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=1-(g(z))^2$\n",
    "\n",
    "$g(z)=\\max(0,z) \\quad \\Longrightarrow \\quad \\dfrac{d}{dz}g(z)=\\left\\{ \\begin{array}{} 0\\, if\\, z \\lt 0 \\\\ 1\\, if\\, z \\geq 0 \\end{array}\\right., \\qquad$ Technically $\\dfrac{d}{dz}g(z)$ is not defined at 0 but in practice it can be \"overlooked\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "-  Parameter Norm Penalties\n",
    "-  Dropout\n",
    "-  Data augmentation\n",
    "-  Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameter norm penalties\n",
    "\n",
    "Let $\\mathcal D=\\{(\\mathbf x^{(1)}, \\mathbf y^{(1)}),...,(\\mathbf x^{(m)}, \\mathbf y^{(m)})\\}$ be a training data set, $\\hat {\\mathbf y(\\mathbf x)}$ the ouput of the network, and $\\Theta$ a set of all parameter tensors in the network $\\boldsymbol \\Theta=\\{\\mathbf W^{[1]},...,\\mathbf W^{[L]},\\mathbf b^{[1]},...,\\mathbf b^{[L]}\\}$.\n",
    "\n",
    "The unregularized cost function is defined as:\n",
    "\n",
    "$J(\\boldsymbol \\Theta)=\\sum_{i=1}^m \\mathcal L (\\hat{\\mathbf y}^{(i)}, \\mathbf y^{(i)}),\\quad$ where $\\mathcal L$ is a per sample loss.\n",
    "\n",
    "We can regularize the cost function by adding the penalty term:\n",
    "\n",
    "$J_{reg}(\\boldsymbol \\Theta)=\\sum_{i=1}^m \\mathcal L (\\hat{\\mathbf y}^{(i)}, \\mathbf y^{(i)}) + \\alpha \\Omega(\\boldsymbol \\Theta)\\quad$ where $\\alpha \\in [0,\\infty]$ is a hyperparameter that weights the relative contribution of the penalty term $\\Omega$.\n",
    "\n",
    "The $L^2$ regularization is the most common parameter norm penalty.\n",
    "\n",
    "$J_{reg}(\\boldsymbol \\Theta)=\\sum_{i=1}^m \\mathcal L (\\hat{\\mathbf y}^{(i)}, \\mathbf y^{(i)}) + \\dfrac{\\lambda}{2m}\\sum_{l=1}^L \\Vert \\mathbf W^{[l]} \\Vert_F^2$\n",
    "\n",
    "Where $\\Vert \\mathbf W^{[l]} \\Vert_F^2=\\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2\\,$ is a __Frobenius norm__ of $\\mathbf W^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout\n",
    "\n",
    "__Dropout__ provides a computationally inexpensive but powerful method of regularizing a broad family of models.\n",
    "\n",
    "![Dropout](images/dropout.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalizing training sets\n",
    "\n",
    "Let $\\mathbf X=\\{\\mathbf x^{(1)},...,\\mathbf x^{(m)}\\}$ be a training data set where $\\mathbf x=\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}$\n",
    "\n",
    "The most common normalization technique is standarization:\n",
    "\n",
    "$ x_i^{'}=\\dfrac{x_i - \\mu_i}{\\sigma_i^2}\\quad$ where $\\mu_i=\\dfrac{1}{m}\\sum_{j=1}^{m}x_i^{(j)}\\quad$ and $\\sigma_i^2=\\dfrac{1}{m}\\sum_{j=1}^{m}(x_i^{(j)} - \\mu_i)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization algorithms\n",
    "\n",
    "1. Mini-batch gradient descent\n",
    "2. Gradient descent with momentum\n",
    "3. RMSProp\n",
    "4. ADAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "To simplify the notation, the following formulas show how to update a single parameter tensor. When  working with sets of parameter tensors the same logic is applied to each tensor.\n",
    "\n",
    "On each iteration:\n",
    "\n",
    "$\\qquad$ Compute gradients $\\nabla J(\\boldsymbol \\Theta)$ on a current minibatch\n",
    "\n",
    "$\\qquad$ Update the parameters using the following formulas: \n",
    "\n",
    "$\\qquad\\qquad \\boldsymbol \\Theta \\leftarrow \\boldsymbol \\Theta - \\alpha \\nabla J(\\boldsymbol \\Theta)$\n",
    "\n",
    "$\\alpha$ - learning rate - is a hyperparameter of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent with momentum\n",
    "\n",
    "To simplify the notation, the following formulas show how to update a single parameter tensor. When  working with sets of parameter tensors the same logic is applied to each tensor.\n",
    "\n",
    "Initialize $\\mathbf v_{\\boldsymbol \\Theta}$ with zeros\n",
    "\n",
    "On each iteration:\n",
    "\n",
    "$\\qquad$ Compute gradients $\\nabla J(\\Theta)$ on a current minibatch\n",
    "\n",
    "$\\qquad$ Update the parameters using the following formulas:\n",
    "\n",
    "$\\qquad\\qquad \\mathbf v_{\\boldsymbol \\Theta}= \\beta \\mathbf v_{\\boldsymbol \\Theta} + (1-\\beta)\\nabla J(\\Theta)$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf{\\boldsymbol \\Theta}= \\boldsymbol{\\Theta} - \\alpha \\mathbf v_{\\boldsymbol \\Theta}$\n",
    "\n",
    "\n",
    "$\\alpha$ and $\\beta$ are hyperparameters. \n",
    "\n",
    "\n",
    "$\\alpha$ needs to be tuned.\n",
    "\n",
    "$\\beta$ is usually set to $0.9$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RMSprop (Root Mean Square prop)\n",
    "\n",
    "To simplify the notation, the following formulas show how to update a single parameter tensor. When  working with sets of parameter tensors the same logic is applied to each tensor.\n",
    "\n",
    "Initialize $\\mathbf s_{\\boldsymbol \\Theta}$ with zeros\n",
    "\n",
    "On each iteration:\n",
    "\n",
    "\n",
    "$\\qquad$ Compute gradients $\\nabla J(\\Theta)$ on a current minibatch\n",
    "\n",
    "$\\qquad$ Update the parameters using the following formulas:\n",
    "\n",
    "$\\qquad\\qquad \\mathbf s_{\\boldsymbol \\Theta}= \\beta \\mathbf s_{\\boldsymbol \\Theta} + (1-\\beta)(\\nabla J(\\Theta))^2$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf{\\boldsymbol \\Theta}= \\boldsymbol{\\Theta} - \\alpha \\dfrac{\\nabla J(\\Theta)}{\\sqrt{\\mathbf S_{\\boldsymbol \\Theta}}+\\epsilon}\\quad$ where $\\epsilon$ is a small number added for numerical stability. Usually $\\epsilon=10^{-8}$\n",
    "\n",
    "\n",
    "$\\alpha$ and $\\beta$ are hyperparameters. \n",
    "\n",
    "$\\alpha$ needs to be tuned\n",
    "\n",
    "$\\beta$ is usually set to $0.9$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADAM (Adaptive moment estimation)\n",
    "\n",
    "To simplify the notation, the following formulas show how to update a single parameter tensor. When  working with sets of parameter tensors the same logic is applied to each tensor.\n",
    "\n",
    "Initialize $\\mathbf v_{\\boldsymbol \\Theta}$ and $\\mathbf s_{\\boldsymbol \\Theta}$with zeros\n",
    "\n",
    "On each iteration $t$:\n",
    "\n",
    "$\\qquad$ Compute gradients $\\nabla J(\\Theta)$ on a current minibatch\n",
    "\n",
    "$\\qquad$ Update the parameters using the following formulas:\n",
    "\n",
    "$\\qquad\\qquad \\mathbf v_{\\boldsymbol \\Theta}= \\beta_1 \\mathbf v_{\\boldsymbol \\Theta} + (1-\\beta_1)\\nabla J(\\Theta)$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf v_{\\boldsymbol \\Theta}^{corrected}= \\dfrac{\\mathbf v_{\\boldsymbol \\Theta}}{1-\\beta_1^t}$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf s_{\\boldsymbol \\Theta}= \\beta_2 \\mathbf s_{\\boldsymbol \\Theta} + (1-\\beta_2)(\\nabla J(\\Theta))^2$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf s_{\\boldsymbol \\Theta}^{corrected}= \\dfrac{s_{\\boldsymbol \\Theta}}{1-\\beta_2^t}$\n",
    "\n",
    "$\\qquad\\qquad \\mathbf{\\boldsymbol \\Theta}= \\boldsymbol{\\Theta} - \\alpha \\dfrac{\\mathbf v_{\\boldsymbol \\Theta}^{corrected}}{\\sqrt{\\mathbf s_{\\boldsymbol \\Theta}^{corrected}} +\\epsilon}\\quad$ where $\\epsilon$ is a small number added for numerical stability. Usually $\\epsilon=10^{-8}$\n",
    "\n",
    "\n",
    "$\\alpha$ and $\\beta_1$ $\\beta_2$ are hyperparameters \n",
    "\n",
    "$\\alpha$ needs to be tuned\n",
    "\n",
    "$\\beta_1$ is usually set to $0.9$\n",
    "\n",
    "$\\beta_2$ is usually set to $0.999$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning rate decay\n",
    "\n",
    "As we approach the minimum during optimization it is beneficial to gradually decrease the leraning rate $\\alpha$.\n",
    "\n",
    "There are many formulas that can be used to implement the learning rate decay.\n",
    "\n",
    "Let $j$ be the current epoch number and $\\alpha_j$ the learning rate to be used at the epoch $j$, $\\alpha_0$ be the initial learning rate, and $\\lambda$ be a decay rate.\n",
    "\n",
    "$\\alpha_j=\\dfrac{1}{1+ j\\lambda }\\alpha_0$\n",
    "\n",
    "$\\alpha_j=a^{j}\\alpha_0\\quad$ where $a$ is a constant close to $0$\n",
    "\n",
    "$\\alpha_j=\\dfrac{k}{\\sqrt i}\\alpha_0\\quad$ where $k$ is an arbitrary constant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks (CNNs) are a specialized neural networks for processing data that has a grid-like topology.\n",
    "\n",
    "CNNs have been very successful in image processing and computer vision applications.\n",
    "\n",
    "Convolutional neural networks utilize a specialized type of linear operation called __convolution__.\n",
    "\n",
    "You could define convolutional neural networks as neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution operation\n",
    "\n",
    "In its most general form, convolution is an operation on two functions of a real-valued argument.\n",
    "\n",
    "Let $x(t)$ and $w(t)$ be two functions of $t\\in\\mathbb R$. The __convolution__  operation is defined as:\n",
    "\n",
    "$s(t)=(x \\ast w)(t)= \\int_{-\\infty}^{\\infty} x(a)w(t-a)da$\n",
    "\n",
    "$x$ is often referred to as the __input__ and the second argument $w$ as the __kernel__.\n",
    "\n",
    "If we assume that $x$ and $w$ are only defined on integers $t\\in \\mathbb I$, we can define the __discrete convolution__.\n",
    "\n",
    "$s(t)=(x \\ast w)(t)=\\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution operation on tensors\n",
    "\n",
    "In machine learning, the inputs are usually a tensor of data, and the kernel a tensor of parameters. Since both tensors are finite we evolve the general form of convolution to substitue the infinite summation with a summation over a finite number of tensor elements - it is assumed that the values are zero everywhere but in the finite set of points stored by tensors.\n",
    "\n",
    "We can define a convolution operation on tensors as follows:\n",
    "\n",
    "Let $\\mathbf X$ and $\\mathbf W$ be two-dimensional matrices (rank 2 tensors) .The convolution operation yields another two-dimensional matrix whose elements are defined as:\n",
    "\n",
    "$z_{i,j}=(\\mathbf X \\ast \\mathbf W)_{i,j}=\\sum_m\\sum_n x_{m,n} w_{i-m,j-n}$. \n",
    "\n",
    "Notice that as $m$ and $n$ increase, the index into the input increases, but the index into the kernel descreases. In  essence, the kernel is __flipped__ relative to the input. Flipping the kernel results in a commutative property of a convolution meaning we can equivalently write:\n",
    "\n",
    "$z_{i,j}=(\\mathbf W \\ast \\mathbf X)_{i,j}=\\sum_m\\sum_n x_{i-m,j-n} w_{m,n}$.\n",
    "\n",
    "Most neural network libraries implement a related function called the __cross-correlation__ (which is the same as convolution but without flipping the kernel) but refer to it as __convolution__.\n",
    "\n",
    "$z_{i,j}=(\\mathbf W \\ast \\mathbf X)_{i,j}=\\sum_m\\sum_n x_{i+m,j+n} w_{m,n}$.\n",
    "\n",
    "In machine learning, the output of a convolution is often referred to as a feature map.\n",
    "\n",
    "The convolution (cross-correlation) formulas as defined in this section can be generalized to tensors of an arbitrary rank. In deep learning applications, the most common are 1D, 2D, 3D, and 4D convolutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of the basic convolution function\n",
    "\n",
    "In practical CNN application the input is usually a grid of vector-valued observations. E.g. RGB channels in a color image. Also we usually want to perform multiple convolutions in parallel. This is because a single kernel can extract only one kind of feature, albeit at many spatial locations. Usually we want each layer of the network to extract many kinds of features,at many locations. And finally, we almost always process inputs in a batch mode.\n",
    "\n",
    "As a result CNNs usually process multichannel convolutions. For example. Let input be RGB images encoded as rank 3 tensors (sometimes referred to as volumes) $\\mathbf X\\in \\mathbb R^{h \\times w \\times C_{in}}$ where $x_{i,j,k}$ is a pixel at row $i$, column $j$, and color channel $k$. If we want to convolve the input image with a set of kernels to generate $C_{out}$ feature maps the kernel tensor $\\mathbf W$ needs be rank 4 tensor $\\mathbf W\\in \\mathbf R^{k_h \\times k_w \\times C_{in} \\times C_{out}}$. The output volume of the multichannel convolution is defined as:\n",
    "\n",
    "$\\mathbf Z[:,:,l]=\\sum_{k=1}^{C_{in}}\\mathbf W[:,:,k,l] \\ast \\mathbf X[:,:,k]+\\mathbf b[l]\\quad$ where $l=\\{1,...,C_{out}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN\n",
    "\n",
    "![RNN](images/RNN.PNG)\n",
    "\n",
    "$\\mathbf h^{\\langle t \\rangle}= g(\\mathbf W_{xh}\\mathbf x^{\\langle t \\rangle} + \\mathbf W_{hh}\\mathbf h^{\\langle t-1 \\rangle} + \\mathbf b)$\n",
    "\n",
    "Or when using concatenated matrix $\\mathbf h^{\\langle t \\rangle} =g \\left ( [\\mathbf W_{xh}; \\mathbf W_{hh}]\\begin{bmatrix}\\mathbf x_{\\langle t \\rangle} \\\\  \\mathbf  h^{\\langle t-1 \\rangle} \\end{bmatrix} + \\mathbf b \\right )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradient problem\n",
    "\n",
    "Basic RNNs suffer from the vanishing gradient problem so we tend to use RNN architectures build out of LSTM and GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "![LSTM](images/LSTM.PNG)\n",
    "\n",
    "$\\Gamma_f = \\sigma(\\mathbf W_{xf} \\mathbf x^{\\langle t \\rangle} + \\mathbf W_{hf} \\mathbf h^{\\langle t-1 \\rangle} + \\mathbf b_f)$\n",
    "\n",
    "$\\Gamma_i = \\sigma(\\mathbf W_{xi} \\mathbf x^{\\langle t \\rangle} + \\mathbf W_{hi} \\mathbf h^{\\langle t-1 \\rangle} + \\mathbf b_i)$\n",
    "\n",
    "$\\Gamma_o = \\sigma(\\mathbf W_{xo} \\mathbf x^{\\langle t \\rangle} + \\mathbf W_{ho} \\mathbf h^{\\langle t-1 \\rangle} + \\mathbf b_o)$\n",
    "\n",
    "$\\mathbf g = \\tanh (\\mathbf W_{xg} \\mathbf x^{\\langle t \\rangle} + \\mathbf W_{go} \\mathbf h^{\\langle t-1 \\rangle} + \\mathbf b_g)$\n",
    "\n",
    "$\\mathbf C^{\\langle t \\rangle}=(\\mathbf C^{\\langle t-1 \\rangle}) \\odot \\Gamma_f) \\oplus (\\mathbf g \\odot \\Gamma_i)$\n",
    "\n",
    "$\\mathbf h^{\\langle t \\rangle}= \\Gamma_o \\odot \\tanh (\\mathbf C^{\\langle t \\rangle})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
